{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract, Transform, Load (ETL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Extração (Extract)**\n",
    "\n",
    "A fase de extração envolve a coleta de dados de múltiplas fontes, que podem estar em diferentes formatos como CSV ou JSON. Para extrair os dados, utilizamos funções que lêem esses arquivos e os convertem em um formato manipulável, geralmente dataframes. A função glob é frequentemente utilizada para identificar e listar todos os arquivos com uma extensão específica em um diretório. Por exemplo, *.csv ou *.json retornaria todos os arquivos CSV ou JSON respectivamente.\n",
    "\n",
    "As funções de extração são projetadas para lidar com diferentes formatos de arquivo: CSV, JSON e XML. Cada função lê os dados de um arquivo específico e os converte em um DataFrame do Pandas.\n",
    "\n",
    "- **Extração de CSV**:\n",
    "  ```python\n",
    "  def extract_from_csv(file_to_process): \n",
    "      dataframe = pd.read_csv(file_to_process) \n",
    "      return dataframe \n",
    "  ```\n",
    "\n",
    "- **Extração de JSON**:\n",
    "  ```python\n",
    "  def extract_from_json(file_to_process): \n",
    "      dataframe = pd.read_json(file_to_process, lines=True) \n",
    "      return dataframe \n",
    "  ```\n",
    "\n",
    "- **Extração de XML**:\n",
    "  ```python\n",
    "  def extract_from_xml(file_to_process): \n",
    "      dataframe = pd.DataFrame(columns=[\"name\", \"height\", \"weight\"]) \n",
    "      tree = ET.parse(file_to_process) \n",
    "      root = tree.getroot() \n",
    "      for person in root: \n",
    "          name = person.find(\"name\").text \n",
    "          height = float(person.find(\"height\").text) \n",
    "          weight = float(person.find(\"weight\").text) \n",
    "          dataframe = pd.concat([dataframe, pd.DataFrame([{\"name\":name, \"height\":height, \"weight\":weight}])], ignore_index=True) \n",
    "      return dataframe \n",
    "  ```\n",
    "\n",
    "A função **`extract()`** orquestra a extração de todos os arquivos encontrados nos diretórios:\n",
    "\n",
    "```python\n",
    "def extract(): \n",
    "    extracted_data = pd.DataFrame(columns=['name','height','weight'])\n",
    "    for csvfile in glob.glob(\"*.csv\"): \n",
    "        extracted_data = pd.concat([extracted_data, extract_from_csv(csvfile)], ignore_index=True) \n",
    "    for jsonfile in glob.glob(\"*.json\"): \n",
    "        extracted_data = pd.concat([extracted_data, extract_from_json(jsonfile)], ignore_index=True) \n",
    "    for xmlfile in glob.glob(\"*.xml\"): \n",
    "        extracted_data = pd.concat([extracted_data, extract_from_xml(xmlfile)], ignore_index=True) \n",
    "    return extracted_data \n",
    "```\n",
    "\n",
    "### 2. **Transformação (Transform)**\n",
    "\n",
    "Após a extração, os dados frequentemente precisam ser transformados para atender aos requisitos do sistema de destino ou para facilitar análises futuras. Isso pode incluir conversões de unidades (por exemplo, de polegadas para metros), limpeza de dados, ou mesmo agregações e cálculos específicos.\n",
    "\n",
    "No exemplo dado, a altura em pés é convertida para metros e o peso em libras é convertido para quilogramas. Isso é feito aplicando operações matemáticas nos valores das colunas correspondentes nos dataframes.\n",
    "\n",
    "A função **`transform(data)`** ajusta as unidades de medidas dos dados, convertendo altura de polegadas para metros e peso de libras para quilogramas:\n",
    "\n",
    "```python\n",
    "def transform(data): \n",
    "    data['height'] = round(data.height * 0.0254, 2)\n",
    "    data['weight'] = round(data.weight * 0.45359237, 2)\n",
    "    return data\n",
    "```\n",
    "\n",
    "### 3. **Carregamento (Load)**\n",
    "\n",
    "A última etapa do processo ETL é carregar os dados transformados no sistema de destino. Isso pode ser um banco de dados, um arquivo CSV ou qualquer outro repositório de dados. No exemplo, o dataframe é salvo como um arquivo CSV utilizando a função to_csv do pandas.\n",
    "\n",
    "Os dados transformados são salvos em um arquivo CSV. A função **`load_data()`** executa essa tarefa:\n",
    "\n",
    "```python\n",
    "def load_data(target_file, transformed_data): \n",
    "    transformed_data.to_csv(target_file)\n",
    "```\n",
    "\n",
    "### 4. **Registro de Log (Logging)**\n",
    "\n",
    "Além das três principais funções de ETL, é essencial manter um registro das operações realizadas. Isso geralmente envolve salvar entradas de log com timestamps que indicam quando cada etapa do processo foi iniciada e concluída. No Python, isso pode ser feito utilizando o módulo datetime para capturar o tempo atual e escrever essa informação em um arquivo de log.\n",
    "\n",
    "O processo de log é manipulado pela função **`log_progress(message)`**, que registra o progresso do processo ETL com carimbos de data/hora:\n",
    "\n",
    "```python\n",
    "def log_progress(message): \n",
    "    timestamp_format = '%Y-%h-%d-%H:%M:%S'  # Formato da data e hora\n",
    "    now = datetime.now() \n",
    "    timestamp = now.strftime(timestamp_format) \n",
    "    with open(log_file, \"a\") as f: \n",
    "        f.write(timestamp + ',' + message + '\\n') \n",
    "```\n",
    "\n",
    "### Execução do Processo ETL e Registros de Log\n",
    "\n",
    "O script segue uma ordem sequencial para chamar as funções de ETL e registrar cada etapa:\n",
    "\n",
    "```python\n",
    "log_progress(\"ETL Job Started\")\n",
    "log_progress(\"Extract phase Started\")\n",
    "extracted_data = extract()\n",
    "log_progress(\"Extract phase Ended\")\n",
    "\n",
    "log_progress(\"Transform phase Started\")\n",
    "transformed_data = transform(extracted_data)\n",
    "print(\"Transformed Data\")\n",
    "print(transformed_data)\n",
    "\n",
    "log_progress(\"Transform phase Ended\")\n",
    "log_progress(\"Load phase Started\")\n",
    "load_data(target_file, transformed_data)\n",
    "log_progress(\"Load phase Ended\")\n",
    "\n",
    "log_progress(\"ETL Job Ended\")\n",
    "```\n",
    "\n",
    "Cada chamada para `log_progress()` assegura que você tenha um registro detalhado do progresso e possíveis pontos de falha, melhorando a transparência e a rastreabilidade do processo ETL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O que é Web Scraping?\n",
    "\n",
    "Web scraping é a técnica de extrair dados de websites de forma programática. É útil para coletar dados de várias fontes onde não existe uma API disponível para extrair informações de forma mais direta. Imagine que você queira analisar os melhores jogadores de uma liga de basquete, web scraping pode te ajudar a coletar esses dados rapidamente.\n",
    "\n",
    "### Introdução ao BeautifulSoup e Requests\n",
    "\n",
    "Para começar, precisamos importar duas bibliotecas essenciais:\n",
    "\n",
    "```python\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "```\n",
    "\n",
    "- **Requests**: Utilizada para fazer o download do conteúdo da página da web.\n",
    "- **BeautifulSoup**: Utilizada para analisar e extrair informações do conteúdo HTML.\n",
    "\n",
    "### Obtendo e Analisando uma Página Web\n",
    "\n",
    "Vamos supor que você precise extrair o nome e salário dos jogadores de uma liga de basquete a partir de uma página web:\n",
    "\n",
    "1. **Baixando a página**:\n",
    "   ```python\n",
    "   url = 'http://example.com'\n",
    "   response = requests.get(url)\n",
    "   page = response.text\n",
    "   ```\n",
    "\n",
    "2. **Criando um objeto BeautifulSoup**:\n",
    "   ```python\n",
    "   soup = BeautifulSoup(page, 'html.parser')\n",
    "   ```\n",
    "\n",
    "### Explorando o Objeto BeautifulSoup\n",
    "\n",
    "O objeto `soup` criado permite que você navegue pela estrutura HTML da página como uma estrutura de dados aninhada. Aqui estão algumas maneiras de interagir com ele:\n",
    "\n",
    "- **Acessando tags**: \n",
    "  ```python\n",
    "  title_tag = soup.title\n",
    "  h3_tag = soup.h3\n",
    "  ```\n",
    "\n",
    "- **Navegando pela árvore**:\n",
    "  - **Acessando filhos**:\n",
    "    ```python\n",
    "    bold_tag = soup.b.find()\n",
    "    ```\n",
    "  - **Acessando o pai**:\n",
    "    ```python\n",
    "    parent_tag = bold_tag.parent\n",
    "    ```\n",
    "  - **Acessando irmãos**:\n",
    "    ```python\n",
    "    next_sibling = bold_tag.next_sibling\n",
    "    ```\n",
    "\n",
    "### Utilizando `find_all` para Filtrar Dados\n",
    "\n",
    "`find_all()` é um método poderoso que busca por todas as instâncias de uma tag, com base em filtros que você especifica:\n",
    "\n",
    "```python\n",
    "rows = soup.find_all('tr')\n",
    "for row in rows:\n",
    "    cells = row.find_all('td')\n",
    "    for cell in cells:\n",
    "        print(cell.text)\n",
    "```\n",
    "\n",
    "Esse trecho de código itera sobre todas as linhas e células de uma tabela, extraindo e imprimindo o texto de cada célula.\n",
    "\n",
    "### Aplicando BeautifulSoup em um Site Real\n",
    "\n",
    "Para fazer web scraping de um site real, você seguirá um processo similar:\n",
    "\n",
    "1. **Importe as bibliotecas e baixe a página**.\n",
    "2. **Crie o objeto BeautifulSoup**.\n",
    "3. **Utilize métodos como `find()` e `find_all()` para extrair informações específicas**.\n",
    "\n",
    "Por exemplo, para extrair dados de uma tabela:\n",
    "\n",
    "```python\n",
    "table = soup.find('table')  # Encontre a tabela\n",
    "rows = table.find_all('tr')  # Extraia todas as linhas da tabela\n",
    "\n",
    "for row in rows:\n",
    "    cells = row.find_all('td')  # Encontre todas as células de cada linha\n",
    "    data = [cell.text for cell in cells]  # Extraia o texto de cada célula\n",
    "    print(data)  # Imprima os dados da célula\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REST APIs & HTTP Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos explorar o uso da biblioteca Requests em Python, uma ferramenta poderosa para trabalhar com o protocolo HTTP. A biblioteca Requests torna muito mais fácil enviar requisições HTTP/1.1, sejam elas GET ou POST, entre outras. Ao entender e aplicar esta biblioteca, você poderá interagir com APIs da web ou automatizar a coleta de dados de páginas da internet de maneira eficaz.\n",
    "\n",
    "### Importando a Biblioteca Requests\n",
    "\n",
    "Primeiro, precisamos importar a biblioteca Requests:\n",
    "\n",
    "```python\n",
    "import requests\n",
    "```\n",
    "\n",
    "### Fazendo uma Requisição GET\n",
    "\n",
    "Vamos começar com uma requisição GET, que é usada para solicitar dados de um servidor. É o tipo de requisição mais comum para recuperar informações de páginas da web ou APIs.\n",
    "\n",
    "#### Exemplo de Requisição GET\n",
    "\n",
    "Suponha que queremos pegar o conteúdo da página da IBM:\n",
    "\n",
    "```python\n",
    "response = requests.get('http://www.ibm.com')\n",
    "```\n",
    "\n",
    "Neste ponto, o objeto `response` contém muitas informações sobre a requisição feita:\n",
    "\n",
    "- **Código de Status**:\n",
    "  ```python\n",
    "  print(response.status_code)  # 200 significa sucesso\n",
    "  ```\n",
    "\n",
    "- **Cabeçalhos da Resposta**:\n",
    "  ```python\n",
    "  print(response.headers)\n",
    "  ```\n",
    "\n",
    "  Isso retornará um dicionário dos cabeçalhos HTTP, onde você pode obter várias informações como a data da requisição ou o tipo de conteúdo respondido.\n",
    "\n",
    "- **Conteúdo da Resposta**:\n",
    "  Se o conteúdo da resposta for HTML, você pode acessá-lo com:\n",
    "\n",
    "  ```python\n",
    "  print(response.text[:100])  # Exibe os primeiros 100 caracteres do HTML\n",
    "  ```\n",
    "\n",
    "### Personalizando Requisições GET com Parâmetros\n",
    "\n",
    "Você pode personalizar uma requisição GET para incluir parâmetros de consulta. Por exemplo, ao acessar uma API:\n",
    "\n",
    "```python\n",
    "params = {'name': 'Joseph', 'ID': '123'}\n",
    "response = requests.get('http://httpbin.org/get', params=params)\n",
    "print(response.url)  # Verifica a URL formada com os parâmetros de consulta\n",
    "```\n",
    "\n",
    "Essa URL conterá os parâmetros de consulta que especificamos no dicionário `params`.\n",
    "\n",
    "### Fazendo uma Requisição POST\n",
    "\n",
    "Diferentemente do GET, a requisição POST envia dados para o servidor e é comumente usada para enviar formulários ou fazer upload de arquivos.\n",
    "\n",
    "#### Exemplo de Requisição POST\n",
    "\n",
    "```python\n",
    "payload = {'name': 'Joseph', 'ID': '123'}\n",
    "response = requests.post('http://httpbin.org/post', data=payload)\n",
    "print(response.text)\n",
    "```\n",
    "\n",
    "Neste exemplo, os dados enviados na requisição POST podem ser acessados no corpo da resposta, não na URL como na requisição GET.\n",
    "\n",
    "### Diferenças Entre GET e POST\n",
    "\n",
    "- **GET** é usado principalmente para solicitar dados do servidor e os parâmetros são enviados na URL.\n",
    "- **POST** é usado para enviar dados ao servidor, enviar formulários, etc., e os dados são enviados no corpo da requisição, não na URL.\n",
    "\n",
    "Ao usar o método `.json()` da resposta de uma requisição que retorna dados JSON, você pode convertê-los diretamente para um dicionário Python:\n",
    "\n",
    "```python\n",
    "response = requests.get('http://httpbin.org/get', params={'key': 'value'})\n",
    "data = response.json()\n",
    "print(data)\n",
    "```\n",
    "\n",
    "Este exemplo imprime a resposta JSON convertida em um dicionário Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on: Web scraping and Extracting Data using APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este script Python combina o uso das bibliotecas `requests`, `sqlite3`, `pandas` e `BeautifulSoup` para extrair informações sobre filmes de uma página da web, armazená-las em um DataFrame do pandas, exportar os dados para um arquivo CSV e, por fim, armazenar esses dados em um banco de dados SQLite. Vamos analisar cada parte do código em detalhes:\n",
    "\n",
    "### Importação de Bibliotecas\n",
    "```python\n",
    "import requests\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "```\n",
    "- **requests**: Utilizada para fazer requisições HTTP a um servidor web.\n",
    "- **sqlite3**: Usada para interagir com bancos de dados SQLite.\n",
    "- **pandas**: Biblioteca para manipulação e análise de dados.\n",
    "- **BeautifulSoup**: Facilita a extração de dados de arquivos HTML e XML.\n",
    "\n",
    "### Definição de Variáveis Iniciais\n",
    "```python\n",
    "url = 'https://web.archive.org/web/20230902185655/https://en.everybodywiki.com/100_Most_Highly-Ranked_Films'\n",
    "db_name = 'Movies.db'\n",
    "table_name = 'Top_50'\n",
    "csv_path = '/home/project/top_50_films.csv'\n",
    "df = pd.DataFrame(columns=[\"Average Rank\",\"Film\",\"Year\"])\n",
    "count = 0\n",
    "```\n",
    "- **url**: Endereço da página da qual os dados serão extraídos.\n",
    "- **db_name**: Nome do arquivo do banco de dados SQLite.\n",
    "- **table_name**: Nome da tabela dentro do banco de dados onde os dados serão armazenados.\n",
    "- **csv_path**: Caminho onde o arquivo CSV será salvo.\n",
    "- **df**: DataFrame inicialmente vazio com colunas definidas para armazenar os dados.\n",
    "- **count**: Contador para limitar o número de filmes a serem processados.\n",
    "\n",
    "### Extração de Dados com BeautifulSoup\n",
    "```python\n",
    "html_page = requests.get(url).text\n",
    "data = BeautifulSoup(html_page, 'html.parser')\n",
    "```\n",
    "- Aqui, `requests.get(url)` é usado para fazer uma requisição GET ao URL especificado e `.text` obtém o conteúdo da página em formato de texto. O resultado é então passado para `BeautifulSoup`, que parseia o HTML.\n",
    "\n",
    "### Processamento dos Dados da Tabela\n",
    "```python\n",
    "tables = data.find_all('tbody')\n",
    "rows = tables[0].find_all('tr')\n",
    "```\n",
    "- Extrai todas as instâncias de `<tbody>` do HTML parseado e, em seguida, seleciona a primeira tabela (`tables[0]`) para extrair todas as linhas (`<tr>`) dela.\n",
    "\n",
    "### Loop para Extração e Armazenamento de Dados\n",
    "```python\n",
    "for row in rows:\n",
    "    if count<50:\n",
    "        col = row.find_all('td')\n",
    "        if len(col)!=0:\n",
    "            data_dict = {\"Average Rank\": col[0].contents[0],\n",
    "                         \"Film\": col[1].contents[0],\n",
    "                         \"Year\": col[2].contents[0]}\n",
    "            df1 = pd.DataFrame(data_dict, index=[0])\n",
    "            df = pd.concat([df, df1], ignore_index=True)\n",
    "            count+=1\n",
    "    else:\n",
    "        break\n",
    "```\n",
    "- O loop itera sobre cada linha da tabela para extrair células (`<td>`). Se existirem células, os dados são extraídos e um novo DataFrame (`df1`) é criado para cada linha. Este DataFrame é então concatenado ao DataFrame principal (`df`).\n",
    "- O loop continua até que 50 filmes sejam adicionados ao DataFrame.\n",
    "\n",
    "### Salvar o DataFrame em um Banco de Dados SQLite\n",
    "```python\n",
    "conn = sqlite3.connect(db_name)\n",
    "df.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "conn.close()\n",
    "```\n",
    "- Estabelece uma conexão com o banco de dados SQLite (`Movies.db`).\n",
    "- Usa `df.to_sql` para salvar o DataFrame em uma tabela (`Top_50`) no banco de dados. Se a tabela já existir, ela será substituída.\n",
    "- Fecha a conexão com o banco de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Average Rank                                           Film  Year\n",
      "0             1                                  The Godfather  1972\n",
      "1             2                                   Citizen Kane  1941\n",
      "2             3                                     Casablanca  1942\n",
      "3             4                         The Godfather, Part II  1974\n",
      "4             5                            Singin' in the Rain  1952\n",
      "5             6                                         Psycho  1960\n",
      "6             7                                    Rear Window  1954\n",
      "7             8                                 Apocalypse Now  1979\n",
      "8             9                          2001: A Space Odyssey  1968\n",
      "9            10                                  Seven Samurai  1954\n",
      "10           11                                        Vertigo  1958\n",
      "11           12                                    Sunset Blvd  1950\n",
      "12           13                                   Modern Times  1936\n",
      "13           14                             Lawrence of Arabia  1962\n",
      "14           15                             North by Northwest  1959\n",
      "15           16                                      Star Wars  1977\n",
      "16           17                                       Parasite  2019\n",
      "17           18                               Schindler's List  1993\n",
      "18           19  Lord of the Rings: The Fellowship of the Ring  2001\n",
      "19           20                           Shawshank Redemption  1994\n",
      "20           21                          It's a Wonderful Life  1946\n",
      "21           22                                   Pulp Fiction  1994\n",
      "22           23                              Avengers: Endgame  2019\n",
      "23           24                                    City Lights  1931\n",
      "24           25                One Flew Over the Cuckoo's Nest  1975\n",
      "25           26                                     Goodfellas  1990\n",
      "26           27                        Raiders of the Lost Ark  1981\n",
      "27           28                                   12 Angry Men  1957\n",
      "28           29                       The Silence of the Lambs  1991\n",
      "29           30                                    Taxi Driver  1976\n",
      "30           31                            Saving Private Ryan  1998\n",
      "31           32                     E.T. the Extra Terrestrial  1982\n",
      "32           33                                          Alien  1979\n",
      "33           34              Spider-Man: Into the Spider-verse  2018\n",
      "34           35                                   Blade Runner  1982\n",
      "35           36                               Double Indemnity  1944\n",
      "36           37                                The Dark Knight  2008\n",
      "37           38                               The Wizard of Oz  1939\n",
      "38           39  Star Wars: Episode V- The Empire Strikes Back  1980\n",
      "39           40                                  The Searchers  1956\n",
      "40           41                             Mad Max: Fury Road  2015\n",
      "41           42                                      Inception  2010\n",
      "42           43          Lord of the Rings: Return of the King  2003\n",
      "43           44                                     The Matrix  1999\n",
      "44           45                                     Fight Club  1999\n",
      "45           46                             Back to the Future  1985\n",
      "46           47                          It Happened One Night  1934\n",
      "47           48                The Good, the Bad, and the Ugly  1966\n",
      "48           49              Lord of the Rings: The Two Towers  2002\n",
      "49           50                                  All About Eve  1950\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://web.archive.org/web/20230902185655/https://en.everybodywiki.com/100_Most_Highly-Ranked_Films'\n",
    "db_name = 'Movies.db'\n",
    "table_name = 'Top_50'\n",
    "csv_path = '/home/project/top_50_films.csv'\n",
    "df = pd.DataFrame(columns=[\"Average Rank\",\"Film\",\"Year\"])\n",
    "count = 0\n",
    "\n",
    "html_page = requests.get(url).text\n",
    "data = BeautifulSoup(html_page, 'html.parser')\n",
    "\n",
    "tables = data.find_all('tbody')\n",
    "rows = tables[0].find_all('tr')\n",
    "\n",
    "for row in rows:\n",
    "    if count<50:\n",
    "        col = row.find_all('td')\n",
    "        if len(col)!=0:\n",
    "            data_dict = {\"Average Rank\": col[0].contents[0],\n",
    "                         \"Film\": col[1].contents[0],\n",
    "                         \"Year\": col[2].contents[0]}\n",
    "            df1 = pd.DataFrame(data_dict, index=[0])\n",
    "            df = pd.concat([df,df1], ignore_index=True)\n",
    "            count+=1\n",
    "    else:\n",
    "        break\n",
    "\n",
    "print(df)\n",
    "\n",
    "#df.to_csv(csv_path)\n",
    "\n",
    "conn = sqlite3.connect(db_name)\n",
    "df.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on: Accessing Databases using Python script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importação de Bibliotecas e Conexão com o Banco de Dados\n",
    "\n",
    "```python\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "conn = sqlite3.connect('STAFF.db')\n",
    "```\n",
    "- **`sqlite3`**: Biblioteca usada para criar e gerenciar conexões com bancos de dados SQLite.\n",
    "- **`pandas`**: Biblioteca para manipulação e análise de dados.\n",
    "- A conexão com o banco de dados SQLite chamado `STAFF.db` é estabelecida.\n",
    "\n",
    "### Leitura de Dados e Criação de Tabela\n",
    "\n",
    "```python\n",
    "table_name = 'INSTRUCTOR'\n",
    "attribute_list = ['ID', 'FNAME', 'LNAME', 'CITY', 'CCODE']\n",
    "file_path = '/home/project/INSTRUCTOR.csv'\n",
    "df = pd.read_csv(file_path, names = attribute_list)\n",
    "df.to_sql(table_name, conn, if_exists = 'replace', index =False)\n",
    "print('Table is ready')\n",
    "```\n",
    "- **Definição das colunas**: As colunas para o DataFrame são definidas.\n",
    "- **Leitura do arquivo CSV**: O DataFrame `df` é criado lendo os dados do arquivo `INSTRUCTOR.csv`, onde o parâmetro `names` especifica os nomes das colunas.\n",
    "- **Gravação no SQLite**: O DataFrame é escrito na tabela `INSTRUCTOR` no banco de dados SQLite. Se a tabela já existir, ela é substituída.\n",
    "- **Confirmação de criação da tabela**: Uma mensagem \"Table is ready\" é exibida.\n",
    "\n",
    "### Execução e Impressão de Consultas SQL\n",
    "\n",
    "```python\n",
    "query_statement = f\"SELECT * FROM {table_name}\"\n",
    "query_output = pd.read_sql(query_statement, conn)\n",
    "print(query_statement)\n",
    "print(query_output)\n",
    "\n",
    "query_statement = f\"SELECT FNAME FROM {table_name}\"\n",
    "query_output = pd.read_sql(query_statement, conn)\n",
    "print(query_statement)\n",
    "print(query_output)\n",
    "\n",
    "query_statement = f\"SELECT COUNT(*) FROM {table_name}\"\n",
    "query_output = pd.read_sql(query_statement, conn)\n",
    "print(query_statement)\n",
    "print(query_output)\n",
    "```\n",
    "- **Seleção de dados**: Diferentes consultas SQL são executadas para selecionar e exibir dados da tabela `INSTRUCTOR`. A primeira consulta seleciona todos os dados, a segunda apenas os nomes e a terceira conta o número de registros.\n",
    "- **`pd.read_sql`**: A função `read_sql` do pandas executa a consulta SQL e retorna os resultados como um DataFrame.\n",
    "\n",
    "### Adição de Novos Dados\n",
    "\n",
    "```python\n",
    "data_dict = {'ID' : [100],\n",
    "             'FNAME' : ['John'],\n",
    "             'LNAME' : ['Doe'],\n",
    "             'CITY' : ['Paris'],\n",
    "             'CCODE' : ['FR']}\n",
    "data_append = pd.DataFrame(data_dict)\n",
    "data_append.to_sql(table_name, conn, if_exists = 'append', index =False)\n",
    "print('Data appended successfully')\n",
    "```\n",
    "- **Criação de novos dados**: Um novo registro é definido como um dicionário e convertido em um DataFrame.\n",
    "- **Adição ao banco de dados**: O novo registro é adicionado à tabela `INSTRUCTOR` usando o modo `append`, que adiciona os dados sem substituir os existentes.\n",
    "- **Confirmação de adição de dados**: Uma mensagem \"Data appended successfully\" é exibida.\n",
    "\n",
    "### Fechamento da Conexão\n",
    "\n",
    "```python\n",
    "conn.close()\n",
    "```\n",
    "- **Encerramento da conexão**: A conexão com o banco de dados é fechada para liberar recursos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Project: Extract, Transform and Load GDP data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este script é um exemplo clássico de um processo ETL (Extract, Transform, Load) usando Python para manipular e armazenar dados de PIB de diferentes países obtidos de uma página da Wikipedia. Ele utiliza bibliotecas como `BeautifulSoup` para scraping web, `pandas` para manipulação de dados, `sqlite3` para operações de banco de dados, e implementa funções customizadas para cada etapa do processo ETL.\n",
    "\n",
    "### Importação de Bibliotecas e Definição de Variáveis Globais\n",
    "\n",
    "```python\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "from datetime import datetime \n",
    "```\n",
    "\n",
    "- **BeautifulSoup**: Utilizado para fazer parsing do HTML e extrair dados.\n",
    "- **requests**: Para fazer requisições HTTP.\n",
    "- **pandas e numpy**: Para manipulação de dados.\n",
    "- **sqlite3**: Para operações de banco de dados.\n",
    "- **datetime**: Para registrar logs com timestamps.\n",
    "\n",
    "### Definição de Variáveis de Configuração\n",
    "```python\n",
    "url = 'https://web.archive.org/web/20230902185326/https://en.wikipedia.org/wiki/List_of_countries_by_GDP_%28nominal%29'\n",
    "table_attribs = [\"Country\", \"GDP_USD_millions\"]\n",
    "db_name = 'World_Economies.db'\n",
    "table_name = 'Countries_by_GDP'\n",
    "csv_path = './Countries_by_GDP.csv'\n",
    "```\n",
    "Essas variáveis armazenam as informações necessárias para a localização dos dados na web, bem como os caminhos para salvar os dados extraídos.\n",
    "\n",
    "### Função `extract`\n",
    "\n",
    "```python\n",
    "def extract(url, table_attribs):\n",
    "    html_page = requests.get(url).text\n",
    "    data = BeautifulSoup(html_page, 'html.parser')\n",
    "    df = pd.DataFrame(columns=table_attribs)\n",
    "\n",
    "    tables = data.find_all('tbody')\n",
    "    rows = tables[2].find_all('tr')\n",
    "\n",
    "    for row in rows:\n",
    "        col = row.find_all('td')\n",
    "        if len(col) != 0:\n",
    "            if col[0].find('a') is not None and '—' not in col[2]:\n",
    "                data_dict = {\"Country\": col[0].a.contents[0],\n",
    "                             \"GDP_USD_millions\": col[2].contents[0]}\n",
    "                df1 = pd.DataFrame(data_dict, index=[0])\n",
    "                df = pd.concat([df, df1], ignore_index=True)\n",
    "\n",
    "    return df\n",
    "```\n",
    "- A função inicia fazendo uma requisição HTTP para obter a página HTML.\n",
    "- Usa `BeautifulSoup` para parsear o HTML e localizar as tabelas dentro da página. O script então foca na terceira tabela (`tables[2]`), assumindo que esta contém os dados de interesse.\n",
    "- Itera sobre cada linha (`tr`) da tabela, e para cada linha, extrai as células (`td`). Se a célula contém um link (`a`) e o valor do PIB não é um traço (indicando dados ausentes), os dados são extraídos e adicionados a um DataFrame temporário `df1`, que é então concatenado ao DataFrame principal `df`.\n",
    "\n",
    "### Função `transform`\n",
    "\n",
    "```python\n",
    "def transform(df):\n",
    "    df['GDP_USD_millions'] = df['GDP_USD_millions'].replace('[\\$,]', '', regex=True).astype(float)\n",
    "    df['GDP_USD_millions'] = round(df['GDP_USD_millions'] / 1000, 2)\n",
    "    df.rename(columns={'GDP_USD_millions': 'GDP_USD_billions'}, inplace=True)\n",
    "    return df\n",
    "```\n",
    "- Esta função primeiro limpa e converte os valores de PIB, que estão em formato de texto com símbolos de moeda e vírgulas, para um tipo numérico (float).\n",
    "- Em seguida, converte os valores de milhões para bilhões e arredonda para duas casas decimais.\n",
    "- Renomeia a coluna para refletir que os valores agora estão em bilhões.\n",
    "\n",
    "### Função `load_to_csv`\n",
    "\n",
    "```python\n",
    "def load_to_csv(df, csv_path):\n",
    "    df.to_csv(csv_path, index=False)\n",
    "```\n",
    "- Simplesmente salva o DataFrame transformado como um arquivo CSV no caminho especificado.\n",
    "\n",
    "### Função `load_to_db`\n",
    "\n",
    "```python\n",
    "def load_to_db(df, sql_connection, table_name):\n",
    "    df.to_sql(table_name, sql_connection, if_exists='replace', index=False)\n",
    "```\n",
    "- Carrega o DataFrame transformado em uma tabela no banco de dados SQLite especificado. Se a tabela já existir, ela será substituída.\n",
    "\n",
    "### Função `run_query`\n",
    "\n",
    "```python\n",
    "def run_query(query_statement, sql_connection):\n",
    "    print(query_statement)\n",
    "    query_output = pd.read_sql(query_statement, sql_connection)\n",
    "    print(query_output)\n",
    "```\n",
    "- Executa uma consulta SQL utilizando a conexão ao banco de dados especificada e imprime tanto a consulta quanto o resultado.\n",
    "\n",
    "### Função `log_progress`\n",
    "\n",
    "```python\n",
    "def log_progress(message):\n",
    "    timestamp_format = '%Y-%h-%d-%H:%M:%S'\n",
    "    now = datetime.now()\n",
    "    timestamp = now.strftime(timestamp_format)\n",
    "    with open(\"./etl_project_log.txt\", \"a\") as f:\n",
    "        f.write(timestamp + ',' + message + '\\n')\n",
    "```\n",
    "- Registra uma mensagem de log com timestamp no arquivo `etl_project_log.txt`. Este log ajuda a rastrear o progresso do processo ETL.\n",
    "\n",
    "### Execução Principal\n",
    "\n",
    "```python\n",
    "log_progress('Preliminaries complete. Initiating ETL process')\n",
    "df = extract(url, table_attribs)\n",
    "\n",
    "log_progress('Data extraction complete. Initiating Transformation process')\n",
    "df = transform(df)\n",
    "\n",
    "log_progress('Data transformation complete. Initiating loading process')\n",
    "load_to_csv(df, csv_path)\n",
    "\n",
    "log_progress('Data saved to CSV file')\n",
    "sql_connection = sqlite3.connect('World_Economies.db')\n",
    "log_progress('SQL Connection initiated.')\n",
    "\n",
    "load_to_db(df, sql_connection, table_name)\n",
    "log_progress('Data loaded to Database as table. Running the query')\n",
    "\n",
    "query_statement = f\"SELECT * from {table_name} WHERE GDP_USD_billions >= 100\"\n",
    "run_query(query_statement, sql_connection)\n",
    "\n",
    "log_progress('Process Complete.')\n",
    "sql_connection.close()\n",
    "```\n",
    "- Este bloco de código controla a execução de todo o processo ETL, chamando as funções definidas anteriormente e registrando cada etapa. Inicia com a extração dos dados, passa pela transformação, salva os dados em um CSV, carrega no banco de dados, executa uma consulta para verificar os dados e, finalmente, fecha a conexão com o banco de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Style Guide and Coding Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importância de Escrever Código Legível\n",
    "Escrever código legível é essencial porque garante que sua equipe possa entender, manter e modificar facilmente o código-base. Um código legível reduz a complexidade de entender o que o código faz, tornando a depuração e os aprimoramentos mais gerenciáveis. Isso é especialmente crucial em ambientes colaborativos onde vários desenvolvedores trabalham no mesmo código.\n",
    "\n",
    "### Convenções de Codificação em Python: PEP8\n",
    "PEP8, ou Proposta de Melhoria do Python número 8, é um documento lançado pelo Python.org que detalha como formatar o código Python para máxima legibilidade. Aqui estão algumas diretrizes-chave do PEP8:\n",
    "\n",
    "1. **Usar Espaços em Vez de Tabs para Indentação**: \n",
    "   - Por quê? Diferentes IDEs e editores de texto podem interpretar tabs de maneira diferente (por exemplo, alguns podem tratar um tab como três espaços, enquanto outros como quatro). Essa inconsistência pode levar a erros de formatação. Usar espaços garante que o código pareça o mesmo em qualquer editor.\n",
    "   - O PEP8 especifica o uso de quatro espaços por nível de indentação para manter o código visualmente estruturado e legível.\n",
    "\n",
    "2. **Linhas em Branco**:\n",
    "   - Use linhas em branco para separar funções e classes para delinear claramente diferentes seções do código. Por exemplo, garantir que haja uma linha em branco entre o final de uma função e o início de uma definição de classe ajuda a identificar onde um bloco termina e outro começa.\n",
    "\n",
    "3. **Espaços ao Redor de Operadores e Após Vírgulas**:\n",
    "   - Inclua espaços ao redor de operadores e após vírgulas para tornar o código mais legível. Por exemplo, escrever `a = b + c` em vez de `a=b+c` torna mais fácil distinguir entre variáveis e operadores.\n",
    "\n",
    "### Convenções de Codificação para Consistência e Gerenciabilidade\n",
    "Para manter um código-base consistente e gerenciável, adira a estas práticas:\n",
    "\n",
    "1. **Decomposição Funcional**:\n",
    "   - Divida grandes blocos de código em funções menores. Isso não apenas realça a legibilidade do código, mas também melhora a reutilização e os testes. Por exemplo, defina uma função para tarefas repetitivas e chame-a quando necessário, em vez de reescrever o código várias vezes.\n",
    "\n",
    "2. **Convenções de Nomes**:\n",
    "   - **Funções e Arquivos**: Use letras minúsculas com sublinhados (por exemplo, `calcular_area` em vez de `CalcularArea`). Este estilo é consistente com as convenções de nomenclatura internas e de bibliotecas do Python.\n",
    "   - **Classes**: Use CamelCase (por exemplo, `ProcessadorDeDados`), o que ajuda a diferenciar classes de funções.\n",
    "   - **Constantes**: Use letras maiúsculas com sublinhados separando palavras (por exemplo, `MAX_TENTATIVAS`). Essa convenção sinaliza que o valor não deve mudar.\n",
    "\n",
    "3. **Evitando Sublinhados em Nomes de Pacotes**:\n",
    "   - Embora funções e variáveis usem sublinhados, nomes de pacotes geralmente não os utilizam (por exemplo, `meupacote` em vez de `meu_pacote`). Isso é para garantir compatibilidade e aderir às convenções de empacotamento do Python.\n",
    "\n",
    "### Análise de Código Estática\n",
    "A análise de código estática envolve verificar o código quanto a erros, violações de estilo e potenciais bugs sem executar o programa. É uma etapa crucial para manter a qualidade do código e a conformidade com padrões de codificação como o PEP8.\n",
    "\n",
    "- **Ferramentas**: Use ferramentas como PyLint para analisar seu código. PyLint verifica as diretrizes do PEP8 e outros padrões de codificação comuns para ajudá-lo a identificar áreas que precisam de melhoria.\n",
    "\n",
    "Em conclusão, adotar as diretrizes do PEP8 e práticas de codificação consistentes aumenta a legibilidade e a gerenciabilidade do seu código. Usar ferramentas de análise de código estática ajuda ainda mais nesses esforços, garantindo a conformidade com os padrões de codificação estabelecidos. Essa abordagem beneficia não apenas os desenvolvedores individuais, mas também as equipes, tornando o código-base mais acessível e fácil de gerenciar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O que é Teste Unitário?\n",
    "\n",
    "Teste unitário é um método para validar se unidades de código estão funcionando conforme projetado. Uma unidade é uma parte menor e testável de uma aplicação. Aqui está um exemplo de uma unidade, que possui duas funções, `square` e `doubler`, no arquivo `mymodule.py`. A função `square` é escrita como `def square(numero): return numero ** 2`. Similarmente, o código para a função `doubler` é escrito como `def doubler(numero): return numero * 2`.\n",
    "\n",
    "### Processo de Teste Unitário\n",
    "\n",
    "Durante o desenvolvimento de código, você testará cada unidade. O teste é realizado em duas fases:\n",
    "\n",
    "1. **Teste Local**: Primeiramente, você testará a unidade em seu sistema local. Se o teste falhar, você determinará a razão da falha e corrigirá o problema. Em seguida, você testará a unidade novamente.\n",
    "\n",
    "2. **Teste no Servidor**: Após o teste unitário local passar, você precisará testar a unidade em um ambiente de servidor, como um servidor de teste de integração contínua e entrega contínua (CI/CD). Se a unidade falhar no teste do servidor, você receberá os detalhes da falha, deverá determinar e corrigir o problema. Uma vez que a unidade passe no teste do servidor, ela é integrada à base de código final.\n",
    "\n",
    "### Construindo Testes Unitários\n",
    "\n",
    "Vamos revisar algumas funções de teste para entender como construir testes unitários:\n",
    "\n",
    "1. **Importar a Biblioteca Unittest**:\n",
    "   ```python\n",
    "   import unittest\n",
    "   ```\n",
    "\n",
    "2. **Importar Funções para Testar**:\n",
    "   ```python\n",
    "   from mymodule import square, doubler\n",
    "   ```\n",
    "\n",
    "3. **Construir a Classe de Teste Unitário**:\n",
    "   ```python\n",
    "   class TestMyModule(unittest.TestCase):\n",
    "       def test_square(self):\n",
    "           self.assertEqual(square(2), 4)\n",
    "\n",
    "       def test_doubler(self):\n",
    "           self.assertEqual(doubler(2), 4)\n",
    "   ```\n",
    "\n",
    "   - A classe `TestMyModule` herda de `unittest.TestCase`.\n",
    "   - Métodos que começam com `test_` são reconhecidos automaticamente pelo framework de teste para serem executados.\n",
    "   - `assertEqual()` é usado para verificar se o valor retornado pela função é igual ao esperado.\n",
    "\n",
    "### Executando e Revisando o Teste\n",
    "\n",
    "Após construir os testes, você os executará, e o output mostrará os resultados, indicando se os testes passaram ou falharam. Por exemplo:\n",
    "\n",
    "- **Output de Sucesso**: Mostra que todos os testes passaram, indicando que as funções foram implementadas corretamente.\n",
    "\n",
    "- **Output de Falha**: Mostra onde os testes falharam, permitindo que você veja qual teste não passou e por quê. Por exemplo, se a função `square` estava calculando o cubo em vez do quadrado, o teste falharia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Módulo, Pacote e Biblioteca em Python\n",
    "\n",
    "1. **Módulo Python**:\n",
    "   - Um módulo é um arquivo `.py` contendo definições e declarações de Python, que incluem funções, classes e variáveis. Você pode importar um módulo para outros scripts e notebooks. Por exemplo, se você tem um módulo chamado `module.py` com funções para calcular o quadrado e o dobro de um número, você pode importá-lo e usar essas funções em seu código.\n",
    "\n",
    "2. **Pacote Python**:\n",
    "   - Um pacote é uma coleção de módulos Python organizada em um diretório que inclui um arquivo `__init__.py`. Este arquivo diferencia o pacote de um diretório comum de scripts Python. Por exemplo, um pacote chamado `myproject` pode conter vários módulos e o arquivo `__init__.py`, que organiza esses módulos como um pacote.\n",
    "\n",
    "3. **Biblioteca Python**:\n",
    "   - Uma biblioteca é uma coleção de pacotes ou pode ser um único pacote com funcionalidades amplas. Exemplos de bibliotecas incluem NumPy, PyTorch e Pandas. Estes termos, pacote e biblioteca, são frequentemente usados de forma intercambiável.\n",
    "\n",
    "### Criando um Pacote Python\n",
    "\n",
    "Para criar um pacote Python, siga estes passos:\n",
    "\n",
    "1. **Crie uma Pasta**:\n",
    "   - Nomeie a pasta com o nome do pacote, por exemplo, `myproject`.\n",
    "\n",
    "2. **Crie o Arquivo `__init__.py`**:\n",
    "   - Este arquivo deve ser criado dentro da pasta do pacote e pode ser inicialmente um arquivo vazio ou incluir comandos de importação dos módulos dentro do pacote.\n",
    "\n",
    "3. **Adicione Módulos**:\n",
    "   - Crie os módulos necessários, como `module1.py` e `module2.py`, dentro da pasta do pacote. Esses módulos podem conter funções específicas como `square`, `doubler` e `mean`.\n",
    "\n",
    "4. **Configure o Arquivo `__init__.py`**:\n",
    "   - No arquivo `__init__.py`, adicione código para referenciar os módulos necessários dentro do pacote, por exemplo:\n",
    "     ```python\n",
    "     from .module1 import square, doubler\n",
    "     from .module2 import mean\n",
    "     ```\n",
    "\n",
    "### Verificando um Pacote Python\n",
    "\n",
    "Para verificar se o pacote está funcionando corretamente:\n",
    "\n",
    "1. **Abra um Terminal Bash**:\n",
    "   - Navegue até o diretório onde seu pacote está localizado.\n",
    "\n",
    "2. **Inicie o Interpretador Python**:\n",
    "   - Execute o comando `python` para abrir o interpretador.\n",
    "\n",
    "3. **Importe o Pacote**:\n",
    "   - Digite `import myproject`. Se o comando for executado sem erros, seu pacote foi carregado com sucesso.\n",
    "\n",
    "### Usando um Pacote Python\n",
    "\n",
    "Para usar o pacote em outros scripts, certifique-se de que a pasta do pacote esteja no mesmo diretório do script ou no caminho de pesquisa do Python. Importe as funções necessárias do pacote e use-as como desejado. Por exemplo:\n",
    "\n",
    "```python\n",
    "from myproject.module1 import square, doubler\n",
    "from myproject.module2 import mean\n",
    "\n",
    "print(square(4))  # Saída esperada: 16\n",
    "print(doubler(4))  # Saída esperada: 8\n",
    "print(mean([2, 1, 3]))  # Saída esperada: 2.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
